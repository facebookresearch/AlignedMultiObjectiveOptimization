{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(123456)\n",
    "np.random.seed(123456)\n",
    "\n",
    "# CHOOSE EXMAPLE\n",
    "# switching example \n",
    "EXAMPLE = \"switching\" # \"switching\", \"selction\", or \"approximate_optimum\"\n",
    "FULL_BATCH = True # True for full batch, False for mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 20\n",
    "hidden_dim = 512\n",
    "output_dim = 7\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  \n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  \n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSSES \n",
    "\n",
    "def poly_loss(outputs, y_tensor, H, alpha, epsilon=0.0):\n",
    "    \"\"\"\n",
    "    Custom loss function that implements (outputs-y_tensor)T H (outputs-y_tensor) ** n\n",
    "    Args:\n",
    "        outputs (torch.tensor): Model outputs\n",
    "        y_tensor (torch.tensor): Target tensor\n",
    "        H (torch.tensor): Hessian matrix\n",
    "        alpha (int): Polynomial degree\n",
    "    Returns:\n",
    "        loss (torch.tensor): Custom loss value\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the difference between outputs and target tensor\n",
    "    diff = (outputs - y_tensor + epsilon)# [BATCH, OUTDIM]\n",
    "    # Calculate the quadratic form using the Hessian matrix\n",
    "    quad_form = torch.matmul(diff, H) # [BATCH, OUTDIM]    \n",
    "    quad_form = torch.sum(quad_form * diff, dim=1) # [BATCH]\n",
    "    # Raise the result to the power of n -> effective power = 2n\n",
    "    loss = quad_form ** alpha    \n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "if EXAMPLE == \"switching\":\n",
    "    # switching example\n",
    "    # When the loss is large, we should see a preference for the third loss function, via w3.\n",
    "    # As the loss gets smaller, we should see a preference for the first loss function, via w1.\n",
    "    # We show this switch in the second plot.\n",
    "    H = torch.eye(output_dim)\n",
    "    criterion1 = lambda outputs, y_tensor: poly_loss(outputs, y_tensor, H, alpha=1.0)\n",
    "    criterion2 = lambda outputs, y_tensor: poly_loss(outputs, y_tensor, H, alpha=1.5)\n",
    "    criterion3 = lambda outputs, y_tensor: poly_loss(outputs, y_tensor, H, alpha=2.0)\n",
    "\n",
    "elif EXAMPLE == \"selection\":\n",
    "    # selection example\n",
    "    # Here we expect to see a preference for the first loss function, via w1 \n",
    "    # due to the eigenvalues of the Hessian matrix.\n",
    "    alpha = 1.\n",
    "    H1 = torch.eye(output_dim)\n",
    "    criterion1 = lambda outputs, y_tensor: poly_loss(outputs, y_tensor, H1, alpha)\n",
    "    H2 = 0.01 * torch.eye(output_dim)\n",
    "    H2[0,0] = 1.\n",
    "    criterion2 = lambda outputs, y_tensor: poly_loss(outputs, y_tensor, H2, alpha)\n",
    "    H3 = 0.0001 * torch.eye(output_dim)\n",
    "    H3[0,0] = 1.\n",
    "    criterion3 = lambda outputs, y_tensor: poly_loss(outputs, y_tensor, H3, alpha)\n",
    "\n",
    "elif EXAMPLE == \"approximate_optimum\": \n",
    "    # not the same optimum exactly\n",
    "    # Simialr to the switching example but with only an approximate optimum.\n",
    "    H = torch.eye(output_dim)\n",
    "    criterion1 = lambda outputs, y_tensor: poly_loss(outputs, y_tensor, H, alpha=1.0, epsilon=0.0)\n",
    "    criterion2 = lambda outputs, y_tensor: poly_loss(outputs, y_tensor, H, alpha=1.5, epsilon=0.001)\n",
    "    criterion3 = lambda outputs, y_tensor: poly_loss(outputs, y_tensor, H, alpha=2.0, epsilon=-0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE DATA\n",
    "\n",
    "if FULL_BATCH: # full batch gradient decent\n",
    "    n_gradient_steps = 200 \n",
    "    batch_size = 100 \n",
    "    X = np.repeat(np.random.uniform(-1, 1, (1, batch_size, input_dim)), repeats=n_gradient_steps, axis=0)\n",
    "else: # mini-batch gradient decent\n",
    "    n_gradient_steps = 1000\n",
    "    batch_size = 1000\n",
    "    X = np.random.uniform(-1, 1, (n_gradient_steps, batch_size, input_dim))\n",
    "\n",
    "\n",
    "X_tensors = torch.from_numpy(X).float()\n",
    "net = Net()\n",
    "y_tensors = net(X_tensors).detach() / output_dim * 20. # make outputs larger to show the weight switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pamoo(outputs, loss1, loss2, loss3, w):\n",
    "    \n",
    "    jacobians = []\n",
    "    for loss in [loss1, loss2, loss3]:\n",
    "        grads = torch.autograd.grad(loss, outputs, create_graph=True) # [BATCH, OUTDIM]\n",
    "        jacobians.append(grads[0].sum(dim=0)) # [OUTDIM]\n",
    "    jacobian = torch.stack(jacobians, dim=0) # [NUM_LOSSES, OUTDIM]\n",
    "    jacobian = jacobian.transpose(1,0) # [OUTDIM, NUM_LOSSES]\n",
    "    diff = torch.stack([loss1, loss2, loss3],dim=0) # [NUM_LOSSES]\n",
    "    \n",
    "    A = jacobian.t() @ jacobian # [NUM_LOSSES, NUM_LOSSES]\n",
    "    A += 0.0001 * torch.eye(A.shape[0])\n",
    "    lr = 3e-3\n",
    "\n",
    "    for i in range(1000): \n",
    "        gradient = 2 * diff - 2 * torch.matmul(A, w)  \n",
    "        w = w + lr * gradient  \n",
    "        w = torch.clamp(w, min=1e-6) # projecting w to R+\n",
    "    return w\n",
    "    \n",
    "def camoo(outputs, loss1, loss2, loss3, w):\n",
    "    del w\n",
    "    # Hutchinson approximation to Hessian\n",
    "    num_samples = 10\n",
    "    hessians = []\n",
    "    for loss in [loss1, loss2, loss3]:\n",
    "        # init hessisan\n",
    "        diag_hessian = torch.zeros_like(outputs[0])# [OUTDIM]\n",
    "        grads = torch.autograd.grad(loss, outputs, create_graph=True) # [BATCH, OUTDIM]\n",
    "        for _ in range(num_samples):\n",
    "            # An independent rademacher random variable with the same shape as the parameters of the model\n",
    "            z = 2 * torch.randint_like(outputs, high=2, memory_format=torch.preserve_format) - 1 # [BATCH, OUTDIM]\n",
    "\n",
    "            # This gives the Hessian vector product Hv\n",
    "            h_z = torch.autograd.grad(\n",
    "                grads, [outputs] , grad_outputs=[z], only_inputs=True, retain_graph=True\n",
    "            )[0] # [BATCH, OUTDIM]\n",
    "\n",
    "            # approximate the expected values of z*(H@z), clip negative values\n",
    "            diag_hessian_update = h_z * z / num_samples # [BATCH, OUTDIM]\n",
    "            # when calculating hessian with activations the first dimension is the batch size\n",
    "            # we average over it and multiply by 1/batch_size normalization\n",
    "            diag_hessian_update = diag_hessian_update.sum(dim=0) # [OUTDIM]\n",
    "            diag_hessian += diag_hessian_update # [OUTDIM]\n",
    "\n",
    "        hessians.append(diag_hessian) \n",
    "    hessians = torch.stack(hessians, dim=0) # [NUM_LOSSES, OUTDIM]\n",
    "    \n",
    "    # primal-dual optimization\n",
    "    w = torch.tensor([1.0, 1.0, 1.0])\n",
    "    tau = 0.01\n",
    "    lr = 1 / (2 * hessians.abs().max() + tau).item()\n",
    "    output_dim = outputs[0].size(0)\n",
    "    q = torch.ones(output_dim) / output_dim # [OUTDIM]\n",
    "    \n",
    "    \n",
    "    for i in range(100): # set iterations high to get maximum visualization effect\n",
    "        # Get extra w\n",
    "        A = hessians @ q\n",
    "        logits_A = lr * (A - torch.max(A))\n",
    "        extra_w = torch.pow(w, exponent=1- lr * tau) * torch.exp(logits_A)\n",
    "        extra_w = extra_w / torch.sum(extra_w)\n",
    "\n",
    "        # Get extra q\n",
    "        B = w @ hessians\n",
    "        logits_B = lr * (B - torch.min(B))\n",
    "        extra_q = torch.pow(q, exponent=1- lr*tau) * torch.exp(-logits_B)\n",
    "        extra_q = extra_q / torch.sum(extra_q)\n",
    "\n",
    "        # Update weights via gradient ascent\n",
    "        A = hessians @ extra_q\n",
    "        logits_A = lr * (A - torch.max(A))\n",
    "        w = torch.pow(w, exponent=1- lr*tau) * torch.exp(logits_A)\n",
    "        w = w / torch.sum(w)\n",
    "\n",
    "        # Update q via gradient descent\n",
    "        B = extra_w @ hessians\n",
    "        logits_B = lr * (B - torch.min(B))\n",
    "        q = torch.pow(q, exponent=1- lr*tau) * torch.exp(-logits_B)\n",
    "        q = q / torch.sum(q)\n",
    "    \n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_experiment(method=pamoo, optimizer= torch.optim.SGD, lr=0.005):\n",
    "    # Initialize the neural network with random weights\n",
    "    net_learn = Net()\n",
    "    # Weight initialization\n",
    "    w = torch.tensor([1.0, 1.0, 1.0])\n",
    "    # Define the optimizer\n",
    "    optimizer = optimizer(net_learn.parameters(), lr=lr)\n",
    "    # define mse\n",
    "    mse_loss = nn.MSELoss()\n",
    " \n",
    "    # Initialize dictionaries to store results\n",
    "    results = {\n",
    "        'weighted_losses': [],\n",
    "        'MSE': [],\n",
    "        'losses': {'loss1': [], 'loss2': [], 'loss3': []},\n",
    "        'weights': {'w1': [], 'w2': [], 'w3': []}\n",
    "    }\n",
    "    \n",
    "    # Train the network\n",
    "    for _, (X_tensor, y_tensor) in tqdm(enumerate(zip(X_tensors, y_tensors)), total=len(X_tensors), desc='Training'):\n",
    "        # Forward pass\n",
    "        outputs = net_learn(X_tensor)\n",
    "        loss1 = criterion1(outputs, y_tensor)\n",
    "        loss2 = criterion2(outputs, y_tensor)\n",
    "        loss3 = criterion3(outputs, y_tensor)\n",
    "        mse = mse_loss(outputs, y_tensor)\n",
    "        # Update weights using the specified method\n",
    "        if method:\n",
    "            with torch.no_grad():\n",
    "                w = method(outputs, loss1, loss2, loss3, w)\n",
    "        # Compute weighted loss\n",
    "        weighted_loss = w[0] * loss1 + w[1] * loss2 + w[2] * loss3\n",
    "        optimizer.zero_grad()\n",
    "        weighted_loss.backward()\n",
    "        optimizer.step()\n",
    "        # Log results\n",
    "        results['weighted_losses'].append(weighted_loss.item())\n",
    "        results['MSE'].append(mse.item())\n",
    "        results['losses']['loss1'].append(loss1.item())\n",
    "        results['losses']['loss2'].append(loss2.item())\n",
    "        results['losses']['loss3'].append(loss3.item())\n",
    "        results['weights']['w1'].append(w[0].item())\n",
    "        results['weights']['w2'].append(w[1].item())\n",
    "        results['weights']['w3'].append(w[2].item())\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "sgd_results = run_experiment(method=None, optimizer= torch.optim.SGD,lr=0.0005)\n",
    "camoo_results = run_experiment(method=camoo,optimizer= torch.optim.SGD,lr=0.0015) # camoo lr is large because the weights get normalized to 1, hence lr = num_losses * lr\n",
    "pamoo_results = run_experiment(method=pamoo,optimizer= torch.optim.SGD,lr=0.0005)\n",
    "adam_results = run_experiment(method=None, optimizer= torch.optim.Adam,lr=0.005)\n",
    "cadam_results = run_experiment(method=camoo, optimizer= torch.optim.Adam,lr=0.015)\n",
    "padam_results = run_experiment(method=pamoo, optimizer= torch.optim.Adam,lr=0.005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results_dict, filename):\n",
    "    # Create a figure with 2 rows and 2 columns\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    # Plot individual losses\n",
    "    for method_name, results in results_dict.items():\n",
    "        axs[0, 0].plot(results['losses']['loss1'], label=method_name)\n",
    "        axs[0, 1].plot(results['losses']['loss2'], label=method_name)\n",
    "        axs[1, 0].plot(results['losses']['loss3'], label=method_name)\n",
    "        axs[1, 1].plot(results['weighted_losses'], label=method_name)\n",
    "    \n",
    "    # Set titles\n",
    "    if EXAMPLE == \"switching\":\n",
    "        axs[0, 0].set_title('$f_1(x)=((y-t)^T H (y-t))^1$')\n",
    "        axs[0, 1].set_title('$f_2(x)=(y-t)^T H (y-t)^{1.5}$')\n",
    "        axs[1, 0].set_title('$f_3(x)=((y-t)^T H (y-t))^2$')\n",
    "    elif EXAMPLE == \"selection\":\n",
    "        axs[0, 0].set_title('$f_1(x)=(y-t)^T H_{1} (y-t)$')\n",
    "        axs[0, 1].set_title('$f_2(x)=(y-t)^T H_{0.01} (y-t)$')\n",
    "        axs[1, 0].set_title('$f_3(x)=(y-t)^T H_{0.0001} (y-t)$')\n",
    "    elif EXAMPLE == \"approximate_optimum\":\n",
    "        axs[0, 0].set_title('$f_1(x)=((y-t)^T H (y-t))$')\n",
    "        axs[0, 1].set_title('$f_2(x)=((y-t+\\epsilon)^T H (y-t+\\epsilon))$')\n",
    "        axs[1, 0].set_title('$f_3(x)=((y-t-\\epsilon)^T H (y-t-\\epsilon))$')\n",
    "    axs[1, 1].set_title('$f(x)= \\sum w_i f_i(x)$')\n",
    "\n",
    "    \n",
    "    # Set y-axis to log scale\n",
    "    for ax in axs.flat:\n",
    "        ax.set_yscale('log')\n",
    "        ax.legend()\n",
    "        #ax.set_ylim(1e-1, 1e0)\n",
    "    \n",
    "    # Layout so plots do not overlap\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "results_dict = {\n",
    "    'EW-SGD': sgd_results,\n",
    "    'CAMOO-SGD':camoo_results,\n",
    "    'PAMOO-SGD': pamoo_results,\n",
    "    'EW-ADAM': adam_results,\n",
    "    'CAMOO-ADAM': cadam_results,\n",
    "    'PAMOO-ADAM': padam_results,\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "plot_results(results_dict, 'all_plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_weights(results, filename):\n",
    "    # Create a figure and axis\n",
    "    fig, ax1 = plt.subplots()\n",
    "    # Plot average loss on left y-axis\n",
    "    ax1.plot(results['MSE'], color='blue')\n",
    "    ax1.set_xlabel('Gradient Steps')\n",
    "    ax1.set_ylabel('Mean Squared Error', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax1.grid(False)  # Turn off grid for left subplot\n",
    "    # ax1.set_yscale('log')\n",
    "    # Create a new y-axis on the right\n",
    "    ax2 = ax1.twinx()\n",
    "    colors = plt.cm.Reds(np.linspace(0.5, 1, 3))  # Generate 3 different red tones\n",
    "    if EXAMPLE == \"switching\":\n",
    "        ax2.plot(results['weights']['w1'], color=colors[0], label='$w_1$  scales $f_1(x)=((y-t)^T H (y-t))$')\n",
    "        ax2.plot(results['weights']['w2'], color=colors[1], label='$w_2$  scales $f_2(x)=(y-t)^T H (y-t)^{1.5}$')\n",
    "        ax2.plot(results['weights']['w3'], color=colors[2], label='$w_3$  scales $f_3(x)=((y-t)^T H (y-t))^2$')\n",
    "    elif EXAMPLE == \"selection\":\n",
    "        ax2.plot(results['weights']['w1'], color=colors[0], label='$w_1$  scales $f_1(x)=(y-t)^T H_{1} (y-t)$')\n",
    "        ax2.plot(results['weights']['w2'], color=colors[1], label='$w_2$  scales $f_2(x)=(y-t)^T H_{0.01} (y-t)$')\n",
    "        ax2.plot(results['weights']['w3'], color=colors[2], label='$w_3$  scales $f_3(x)=(y-t)^T H_{0.0001} (y-t)$')\n",
    "    elif EXAMPLE == \"approximate_optimum\":\n",
    "        ax2.plot(results['weights']['w1'], color=colors[0], label='$w_1$  scales $f_1(x)=((y-t)^T H (y-t))$')\n",
    "        ax2.plot(results['weights']['w2'], color=colors[1], label='$w_2$  scales $f_2(x)=((y-t+\\epsilon)^T H (y-t+\\epsilon))$')\n",
    "        ax2.plot(results['weights']['w3'], color=colors[2], label='$w_3$  scales $f_3(x)=((y-t-\\epsilon)^T H (y-t-\\epsilon))$')\n",
    "    ax2.set_ylabel('Weight for respective Loss Function', color='red')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    ax2.grid(False)  # Turn off grid for right subplot\n",
    "    # Add legend\n",
    "    ax2.legend(loc='upper right')\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plot_weights(results_dict['PAMOO-SGD'],'weights_PAMOO-SGD')\n",
    "# plot_weights(results_dict['PAMOO-ADAM'],'weights_PAMOO-ADAM')\n",
    "plot_weights(results_dict['CAMOO-SGD'], 'weights_CAMOO-SGD')\n",
    "# plot_weights(results_dict['CAMOO-ADAM'], 'weights_CAMOO-ADAM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results_dict,filename):\n",
    "    # Create a figure with 2 rows and 2 columns\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(6, 4))\n",
    "    \n",
    "    # Plot individual losses\n",
    "    for method_name, results in results_dict.items():\n",
    "        axs.plot(results['MSE'], label=method_name)\n",
    "\n",
    "    axs.set_ylabel('Mean Squared Error')\n",
    "    axs.set_xlabel('Gradient Steps')\n",
    "    # set limits if needed\n",
    "    # axs.set_ylim(1e-3, 2e0)\n",
    "    # axs.set_xlim(0, 200)\n",
    "    \n",
    "    # Set y-axis to log scale\n",
    "    axs.set_yscale('log')\n",
    "    axs.legend()\n",
    "\n",
    "    # Layout so plots do not overlap\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "results_dict = {\n",
    "    'EW-SGD': sgd_results,\n",
    "    'CAMOO-SGD':camoo_results,\n",
    "    'PAMOO-SGD': pamoo_results,\n",
    "    'EW-ADAM': adam_results,\n",
    "    'CAMOO-ADAM': cadam_results,\n",
    "    'PAMOO-ADAM': padam_results,\n",
    "    \n",
    "}\n",
    "\n",
    "plot_results(results_dict, 'MSE_plot')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
